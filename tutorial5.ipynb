{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee3b654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "height = 28\n",
    "width = 28\n",
    "batch_size = 128 # what does this effect\n",
    "data_path = '/tmp/data/mnist'\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72b336e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output, _ = net(data.view(batch_size, -1))\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, train=False)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52ac5722",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((height, width)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "# what size was the data before?\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e747bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de6a3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = height * width\n",
    "num_hidden = 1000\n",
    "num_outputs = 10\n",
    "\n",
    "# Temporal Dynamics\n",
    "# experiment setting these to different values\n",
    "time_steps = 25\n",
    "beta = 0.95\n",
    "\n",
    "# why layers and not individual neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da4a1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch = iter(train_loader)\n",
    "# meta_counter = 0\n",
    "# for data, targets in train_batch:\n",
    "#     print(data.view(batch_size,-1))\n",
    "#     print(data.view(batch_size,-1).size())\n",
    "#     print(data.size())\n",
    "#     counter = 0\n",
    "#     # data is [128,1,28,28]\n",
    "#     # 128 28 by 28 images \n",
    "#     # targets is a list of numeric labels\n",
    "#     print('target size', targets.size(), targets)\n",
    "#     for i in data:\n",
    "#         print(i.size())\n",
    "#         counter += 1\n",
    "#         # i[0] is image\n",
    "#         # i is [i[0]]\n",
    "#         # plt.figure()\n",
    "#         # plt.imshow(i[0])\n",
    "#         if counter > 5:\n",
    "#             break\n",
    "#     meta_counter += 1\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0507d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = time_steps\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "\n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "temporal_loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fa0130",
   "metadata": {},
   "source": [
    "# define training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7acea119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         print(\"INIT WAS CALLED\")\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "#         self.lif1 = snn.Leaky(beta=beta)\n",
    "#         self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "#         self.lif2 = snn.Leaky(beta=beta)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # print(x.size())\n",
    "#         # arr = []\n",
    "#         # temp = []\n",
    "#         # for num, ele in enumerate(x[0]):\n",
    "#         #     if num != 0 and num % 28 == 0:\n",
    "#         #         arr.append(temp)\n",
    "#         #         temp = []\n",
    "#         #     temp.append(ele)\n",
    "#         # plt.figure()\n",
    "#         # plt.imshow(arr)\n",
    "#         # x is 128 images, whose dimensions have been flattened to 1D (784 = 28*24)\n",
    "#         # print(\"FORWARD WAS CALLED, returns memory & spike\")\n",
    "#         mem1 = self.lif1.init_leaky() # initialize hidden states\n",
    "#         # must have to do this for each leaky neuron\n",
    "#         mem2 = self.lif2.init_leaky()\n",
    "#         spk2_rec = []\n",
    "#         mem2_rec = []\n",
    "#         for step in range(time_steps):\n",
    "#             cur1 = self.fc1(x)\n",
    "#             # 128 x 1000 128 images, 1D stretched ig (1000 ???)\n",
    "#             # it's passing the same value at every time step\n",
    "#             # this is what it meant by rate encoding i think\n",
    "#             spk1, mem1 = self.lif1(cur1, mem1)\n",
    "#             cur2 = self.fc2(spk1)\n",
    "#             spk2, mem2 = self.lif2(cur2, mem2)\n",
    "#             # a single pass through\n",
    "#             spk2_rec.append(spk2)\n",
    "#             mem2_rec.append(mem2)\n",
    "#             # spk2_rec_np = torch.tensor([[t.item() for t in row] for row in spk2_rec])\n",
    "#             # spk2_rec = 25 x 128 x 10 ... what is the 10??\n",
    "#             # oh like what output neuron spiked... interesting ...\n",
    "#             # print('membrane potential size', np.array(mem2_rec).size())\n",
    "#         # print(spk2_rec[0][0], spk2_rec[0][1],spk2_rec[1][0])\n",
    "\n",
    "#         return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "# net = Net().to(device) # cuda probably not available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf93785",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab10cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 1\n",
    "# loss_hist = []\n",
    "# test_loss_hist = []\n",
    "# counter = 0\n",
    "\n",
    "# # Outer training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     iter_counter = 0\n",
    "#     train_batch = iter(train_loader)\n",
    "\n",
    "#     # Minibatch training loop\n",
    "#     for data, targets in train_batch:\n",
    "#         # data is 128x1x28x28\n",
    "#         # targets is 128\n",
    "#         data = data.to(device)\n",
    "#         targets = targets.to(device)\n",
    "\n",
    "#         # sets training mode\n",
    "#         net.train()\n",
    "#         # forward pass\n",
    "#         spk_rec, mem_rec = net(data.view(batch_size, -1)) # data.view = 128x784\n",
    "\n",
    "#         # initialize the loss & sum over time\n",
    "#         loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "#         for step in range(time_steps): # for each time step calculate loss\n",
    "#             # inside the forward pass, each membrane potential is saved @ step index\n",
    "#             # mem_rec & spk_rec are 25 x 128 x 10\n",
    "#             # 10 output neurons, must be membrane potential of output neurons...\n",
    "#             # what is like the threshold here\n",
    "#             # what should be the target?\n",
    "#             # if it's arranged T x B x output neurons\n",
    "#             temporal_threshold = 0\n",
    "#             if step < temporal_threshold:\n",
    "#                 target_zeros = torch.zeros_like(spk_rec[step])\n",
    "#                 loss_val += temporal_loss(spk_rec[step], target_zeros)\n",
    "#                 \"\"\"\n",
    "#                 have to pass target spike times\n",
    "#                 create an array with the spike times & the target spikes times\n",
    "#                 for each time step ?\n",
    "#                 so for each time step, there are 128 images, and each image has 10 output neurons\n",
    "#                 so like maybe we pass 128 x 10 of 0s for the first 2 time_steps or something\n",
    "#                 \"\"\"\n",
    "#             loss_val += loss(mem_rec[step], targets)\n",
    "#         # if counter % 5 == 0:\n",
    "#         #     print(mem_rec.size())\n",
    "#         #     for i in range(len(spk_rec[0])):\n",
    "#         #         if i < 5:\n",
    "#         #             print(spk_rec[0][i])\n",
    "#         #             print(targets[i])\n",
    "\n",
    "#         # Gradient calculation + weight update\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_val.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Store loss history for future plotting\n",
    "#         loss_hist.append(loss_val.item())\n",
    "\n",
    "#         # Test set\n",
    "#         with torch.no_grad():\n",
    "#             net.eval()\n",
    "#             test_data, test_targets = next(iter(test_loader))\n",
    "#             test_data = test_data.to(device)\n",
    "#             test_targets = test_targets.to(device)\n",
    "\n",
    "#             # Test set forward pass\n",
    "#             test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "#             # Test set loss\n",
    "#             test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "#             for step in range(time_steps):\n",
    "#                 test_loss += loss(test_mem[step], test_targets)\n",
    "#             test_loss_hist.append(test_loss.item())\n",
    "\n",
    "#             # Print train/test loss/accuracy\n",
    "#             if counter % 50 == 0:\n",
    "#                 train_printer()\n",
    "#             counter += 1\n",
    "#             iter_counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e500b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temporal threshold now 3 0.0 0 93\n",
      "Epoch 0, Iteration 0\n",
      "Train Set Loss: 19.82\n",
      "Test Set Loss: 10.29\n",
      "Train set accuracy for a single minibatch: 89.06%\n",
      "Test set accuracy for a single minibatch: 88.28%\n",
      "\n",
      "\n",
      "tensor(-1) tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 1.], grad_fn=<SelectBackward0>)\n",
      "temporal threshold now 3.2 0.010526315789473684 1 95\n",
      "temporal threshold now 3.4000000000000004 0.0 0 93\n",
      "temporal threshold now 3.6000000000000005 0.0 0 87\n",
      "tensor(2) tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "temporal threshold now 3.8000000000000007 0.010869565217391304 1 92\n",
      "temporal threshold now 4.000000000000001 0.0 0 97\n",
      "temporal threshold now 4.200000000000001 0.0 0 90\n",
      "temporal threshold now 4.400000000000001 0.0 0 88\n",
      "temporal threshold now 4.600000000000001 0.0 0 79\n",
      "tensor(2) tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "temporal threshold now 4.800000000000002 0.011235955056179775 1 89\n",
      "temporal threshold now 5.000000000000002 0.0 0 81\n",
      "tensor(2) tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "temporal threshold now 5.200000000000002 0.011627906976744186 1 86\n",
      "temporal threshold now 5.400000000000002 0.0 0 95\n",
      "tensor(2) tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "temporal threshold now 5.600000000000002 0.011235955056179775 1 89\n",
      "tensor(2) tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "temporal threshold now 5.8000000000000025 0.010309278350515464 1 97\n",
      "temporal threshold now 6.000000000000003 0.0 0 82\n",
      "temporal threshold now 6.200000000000003 0.0 0 98\n",
      "temporal threshold now 6.400000000000003 0.0 0 95\n",
      "temporal threshold now 6.600000000000003 0.0 0 86\n",
      "temporal threshold now 6.800000000000003 0.0 0 84\n",
      "temporal threshold now 7.0000000000000036 0.0 0 85\n",
      "tensor(8) tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], grad_fn=<SelectBackward0>)\n",
      "temporal threshold now 7.200000000000004 0.0136986301369863 1 73\n",
      "temporal threshold now 7.400000000000004 0.0 0 83\n",
      "temporal threshold now 7.600000000000004 0.0 0 89\n",
      "temporal threshold now 7.800000000000004 0.0 0 77\n",
      "tensor(7) tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "temporal threshold now 8.000000000000004 0.012345679012345678 1 81\n",
      "temporal threshold now 8.200000000000003 0.0 0 87\n",
      "temporal threshold now 8.400000000000002 0.0 0 82\n",
      "temporal threshold now 8.600000000000001 0.0 0 72\n",
      "temporal threshold now 8.8 0.0 0 87\n",
      "temporal threshold now 9.0 0.0 0 92\n",
      "tensor(2) tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "temporal threshold now 9.2 0.012048192771084338 1 83\n",
      "temporal threshold now 9.399999999999999 0.0 0 82\n",
      "tensor(1) tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "temporal threshold now 9.599999999999998 0.011494252873563218 1 87\n",
      "temporal threshold now 9.799999999999997 0.0 0 81\n",
      "temporal threshold now 9.999999999999996 0.0 0 83\n",
      "tensor(2) tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "temporal threshold now 10.199999999999996 0.011494252873563218 1 87\n",
      "temporal threshold now 10.399999999999995 0.0 0 81\n",
      "temporal threshold now 10.599999999999994 0.0 0 85\n",
      "temporal threshold now 10.799999999999994 0.0 0 77\n",
      "temporal threshold now 10.999999999999993 0.0 0 81\n",
      "temporal threshold now 11.199999999999992 0.0 0 70\n",
      "temporal threshold now 11.399999999999991 0.0 0 69\n",
      "temporal threshold now 11.59999999999999 0.0 0 87\n",
      "tensor(2) tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "temporal threshold now 11.79999999999999 0.014705882352941176 1 68\n",
      "temporal threshold now 11.99999999999999 0.0 0 74\n",
      "temporal threshold now 12.199999999999989 0.0 0 74\n",
      "temporal threshold now 12.399999999999988 0.0 0 82\n",
      "temporal threshold now 12.599999999999987 0.0 0 75\n",
      "temporal threshold now 12.799999999999986 0.0 0 67\n",
      "temporal threshold now 12.999999999999986 0.0 0 70\n",
      "Epoch 0, Iteration 50\n",
      "Train Set Loss: 9.97\n",
      "Test Set Loss: 9.71\n",
      "Train set accuracy for a single minibatch: 85.16%\n",
      "Test set accuracy for a single minibatch: 82.81%\n",
      "\n",
      "\n",
      "tensor(2) tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "temporal threshold now 13.199999999999985 0.014492753623188406 1 69\n",
      "tensor(2) tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "temporal threshold now 13.399999999999984 0.014925373134328358 1 67\n",
      "temporal threshold now 13.599999999999984 0.0 0 74\n",
      "temporal threshold now 13.799999999999983 0.0 0 70\n",
      "temporal threshold now 13.999999999999982 0.0 0 77\n",
      "temporal threshold now 14.199999999999982 0.0 0 66\n",
      "temporal threshold now 14.39999999999998 0.0 0 54\n",
      "temporal threshold now 14.59999999999998 0.0 0 57\n",
      "temporal threshold now 14.79999999999998 0.0 0 55\n",
      "temporal threshold now 14.999999999999979 0.0 0 68\n",
      "temporal threshold now 15.199999999999978 0.0 0 52\n",
      "temporal threshold now 15.399999999999977 0.0 0 45\n",
      "temporal threshold now 15.599999999999977 0.0 0 42\n",
      "temporal threshold now 15.799999999999976 0.0 0 55\n",
      "temporal threshold now 15.999999999999975 0.0 0 43\n",
      "temporal threshold now 16.199999999999974 0.0 0 55\n",
      "temporal threshold now 16.399999999999974 0.0 0 28\n",
      "temporal threshold now 16.599999999999973 0.0 0 51\n",
      "temporal threshold now 16.799999999999972 0.0 0 41\n",
      "temporal threshold now 16.99999999999997 0.0 0 44\n",
      "temporal threshold now 17.19999999999997 0.0 0 28\n",
      "temporal threshold now 17.39999999999997 0.0 0 32\n",
      "temporal threshold now 17.59999999999997 0.0 0 30\n",
      "temporal threshold now 17.79999999999997 0.0 0 23\n",
      "temporal threshold now 17.999999999999968 0.0 0 22\n",
      "temporal threshold now 18.199999999999967 0.0 0 28\n",
      "temporal threshold now 18.399999999999967 0.0 0 26\n",
      "temporal threshold now 18.599999999999966 0.0 0 21\n",
      "temporal threshold now 18.799999999999965 0.0 0 16\n",
      "temporal threshold now 18.999999999999964 0.0 0 20\n",
      "temporal threshold now 19.199999999999964 0.0 0 16\n",
      "temporal threshold now 19.399999999999963 0.0 0 17\n",
      "temporal threshold now 19.599999999999962 0.0 0 21\n",
      "temporal threshold now 19.79999999999996 0.0 0 16\n",
      "temporal threshold now 19.99999999999996 0.0 0 16\n",
      "temporal threshold now 20.19999999999996 0.0 0 6\n",
      "temporal threshold now 20.39999999999996 0.0 0 12\n",
      "temporal threshold now 20.59999999999996 0.0 0 5\n",
      "temporal threshold now 20.799999999999958 0.0 0 5\n",
      "temporal threshold now 20.999999999999957 0.0 0 7\n",
      "temporal threshold now 21.199999999999957 0.0 0 3\n",
      "temporal threshold now 21.399999999999956 0.0 0 10\n",
      "temporal threshold now 21.599999999999955 0.0 0 7\n",
      "temporal threshold now 21.799999999999955 0.0 0 9\n",
      "temporal threshold now 21.999999999999954 0.0 0 3\n",
      "temporal threshold now 22.199999999999953 0.0 0 5\n",
      "temporal threshold now 22.399999999999952 0.0 0 8\n",
      "temporal threshold now 22.59999999999995 0.0 0 4\n",
      "temporal threshold now 22.79999999999995 0.0 0 6\n",
      "temporal threshold now 22.99999999999995 0.0 0 3\n",
      "Epoch 0, Iteration 100\n",
      "Train Set Loss: 5.96\n",
      "Test Set Loss: 7.65\n",
      "Train set accuracy for a single minibatch: 20.31%\n",
      "Test set accuracy for a single minibatch: 16.41%\n",
      "\n",
      "\n",
      "temporal threshold now 23.19999999999995 0.0 0 4\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m         \u001b[38;5;28mprint\u001b[39m(targets[image_idx]-\u001b[32m1\u001b[39m, spk_rec[step][image_idx])\n\u001b[32m     28\u001b[39m         accuracy_count += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtemporal threshold now\u001b[39m\u001b[33m\"\u001b[39m, temporal_threshold, \u001b[43maccuracy_count\u001b[49m\u001b[43m/\u001b[49m\u001b[43mspiked_arrs\u001b[49m, accuracy_count, spiked_arrs)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accuracy_count/spiked_arrs < \u001b[32m.6\u001b[39m:\n\u001b[32m     31\u001b[39m     temporal_threshold += \u001b[32m.2\u001b[39m\n",
      "\u001b[31mZeroDivisionError\u001b[39m: division by zero"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "num_steps = time_steps\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "temporal_threshold = 3\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "        accuracy_count = 0 # accuracy_count/len(spk_rec[step])\n",
    "        spiked_arrs = 0\n",
    "        for image_idx in range(len(spk_rec[step])): # spk_rec[step] = 128 x 10 \n",
    "            if 1 in spk_rec[step][image_idx]: # if any neuron spiked\n",
    "                spiked_arrs+=1\n",
    "            if spk_rec[step][image_idx][targets[image_idx]-1] == 1: # if the correct neuron spiked\n",
    "                print(targets[image_idx]-1, spk_rec[step][image_idx])\n",
    "                accuracy_count += 1\n",
    "        print(\"temporal threshold now\", temporal_threshold, accuracy_count/spiked_arrs, accuracy_count, spiked_arrs)\n",
    "        if accuracy_count/spiked_arrs < .6:\n",
    "            temporal_threshold += .2\n",
    "        if accuracy_count/spiked_arrs > .8:\n",
    "            temporal_threshold -= .2\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        for step in range(num_steps):\n",
    "            if step < temporal_threshold:\n",
    "                # 128 x 10 --> [1,0,0,0,0,0,0,0,0] [0,0,0,0,0,0,0,0,0]\n",
    "                target_zeros = torch.zeros_like(spk_rec[step])\n",
    "                loss_val += temporal_loss(spk_rec[step], target_zeros)\n",
    "            loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "            for step in range(num_steps):\n",
    "                test_loss += loss(test_mem[step], test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            if counter % 50 == 0:\n",
    "                train_printer()\n",
    "            counter += 1\n",
    "            iter_counter +=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
