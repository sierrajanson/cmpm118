{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ee3b654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "height = 28\n",
    "width = 28\n",
    "batch_size = 128 # what does this effect\n",
    "data_path = '/tmp/data/mnist'\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "72b336e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output, _ = net(data.view(batch_size, -1))\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, train=False)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "52ac5722",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((height, width)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "# what size was the data before?\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e747bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "de6a3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = height * width\n",
    "num_hidden = 1000\n",
    "num_outputs = 10\n",
    "\n",
    "# Temporal Dynamics\n",
    "# experiment setting these to different values\n",
    "time_steps = 25\n",
    "beta = 0.95\n",
    "\n",
    "# why layers and not individual neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "da4a1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch = iter(train_loader)\n",
    "# meta_counter = 0\n",
    "# for data, targets in train_batch:\n",
    "#     print(data.view(batch_size,-1))\n",
    "#     print(data.view(batch_size,-1).size())\n",
    "#     print(data.size())\n",
    "#     counter = 0\n",
    "#     # data is [128,1,28,28]\n",
    "#     # 128 28 by 28 images \n",
    "#     # targets is a list of numeric labels\n",
    "#     print('target size', targets.size(), targets)\n",
    "#     for i in data:\n",
    "#         print(i.size())\n",
    "#         counter += 1\n",
    "#         # i[0] is image\n",
    "#         # i is [i[0]]\n",
    "#         # plt.figure()\n",
    "#         # plt.imshow(i[0])\n",
    "#         if counter > 5:\n",
    "#             break\n",
    "#     meta_counter += 1\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a0507d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "temporal_loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fa0130",
   "metadata": {},
   "source": [
    "# define training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7acea119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT WAS CALLED\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        print(\"INIT WAS CALLED\")\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(x.size())\n",
    "        # arr = []\n",
    "        # temp = []\n",
    "        # for num, ele in enumerate(x[0]):\n",
    "        #     if num != 0 and num % 28 == 0:\n",
    "        #         arr.append(temp)\n",
    "        #         temp = []\n",
    "        #     temp.append(ele)\n",
    "        # plt.figure()\n",
    "        # plt.imshow(arr)\n",
    "        # x is 128 images, whose dimensions have been flattened to 1D (784 = 28*24)\n",
    "        # print(\"FORWARD WAS CALLED, returns memory & spike\")\n",
    "        mem1 = self.lif1.init_leaky() # initialize hidden states\n",
    "        # must have to do this for each leaky neuron\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "        for step in range(time_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            # 128 x 1000 128 images, 1D stretched ig (1000 ???)\n",
    "            # it's passing the same value at every time step\n",
    "            # this is what it meant by rate encoding i think\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            # a single pass through\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "            # spk2_rec_np = torch.tensor([[t.item() for t in row] for row in spk2_rec])\n",
    "            # spk2_rec = 25 x 128 x 10 ... what is the 10??\n",
    "            # oh like what output neuron spiked... interesting ...\n",
    "            # print('membrane potential size', np.array(mem2_rec).size())\n",
    "        # print(spk2_rec[0][0], spk2_rec[0][1],spk2_rec[1][0])\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "net = Net().to(device) # cuda probably not available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf93785",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ab10cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 1\n",
    "# loss_hist = []\n",
    "# test_loss_hist = []\n",
    "# counter = 0\n",
    "\n",
    "# # Outer training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     iter_counter = 0\n",
    "#     train_batch = iter(train_loader)\n",
    "\n",
    "#     # Minibatch training loop\n",
    "#     for data, targets in train_batch:\n",
    "#         # data is 128x1x28x28\n",
    "#         # targets is 128\n",
    "#         data = data.to(device)\n",
    "#         targets = targets.to(device)\n",
    "\n",
    "#         # sets training mode\n",
    "#         net.train()\n",
    "#         # forward pass\n",
    "#         spk_rec, mem_rec = net(data.view(batch_size, -1)) # data.view = 128x784\n",
    "\n",
    "#         # initialize the loss & sum over time\n",
    "#         loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "#         for step in range(time_steps): # for each time step calculate loss\n",
    "#             # inside the forward pass, each membrane potential is saved @ step index\n",
    "#             # mem_rec & spk_rec are 25 x 128 x 10\n",
    "#             # 10 output neurons, must be membrane potential of output neurons...\n",
    "#             # what is like the threshold here\n",
    "#             # what should be the target?\n",
    "#             # if it's arranged T x B x output neurons\n",
    "#             temporal_threshold = 0\n",
    "#             if step < temporal_threshold:\n",
    "#                 target_zeros = torch.zeros_like(spk_rec[step])\n",
    "#                 loss_val += temporal_loss(spk_rec[step], target_zeros)\n",
    "#                 \"\"\"\n",
    "#                 have to pass target spike times\n",
    "#                 create an array with the spike times & the target spikes times\n",
    "#                 for each time step ?\n",
    "#                 so for each time step, there are 128 images, and each image has 10 output neurons\n",
    "#                 so like maybe we pass 128 x 10 of 0s for the first 2 time_steps or something\n",
    "#                 \"\"\"\n",
    "#             loss_val += loss(mem_rec[step], targets)\n",
    "#         # if counter % 5 == 0:\n",
    "#         #     print(mem_rec.size())\n",
    "#         #     for i in range(len(spk_rec[0])):\n",
    "#         #         if i < 5:\n",
    "#         #             print(spk_rec[0][i])\n",
    "#         #             print(targets[i])\n",
    "\n",
    "#         # Gradient calculation + weight update\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_val.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Store loss history for future plotting\n",
    "#         loss_hist.append(loss_val.item())\n",
    "\n",
    "#         # Test set\n",
    "#         with torch.no_grad():\n",
    "#             net.eval()\n",
    "#             test_data, test_targets = next(iter(test_loader))\n",
    "#             test_data = test_data.to(device)\n",
    "#             test_targets = test_targets.to(device)\n",
    "\n",
    "#             # Test set forward pass\n",
    "#             test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "#             # Test set loss\n",
    "#             test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "#             for step in range(time_steps):\n",
    "#                 test_loss += loss(test_mem[step], test_targets)\n",
    "#             test_loss_hist.append(test_loss.item())\n",
    "\n",
    "#             # Print train/test loss/accuracy\n",
    "#             if counter % 50 == 0:\n",
    "#                 train_printer()\n",
    "#             counter += 1\n",
    "#             iter_counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9e500b3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# initialize the loss & sum over time\u001b[39;00m\n\u001b[32m     21\u001b[39m loss_val = torch.zeros((\u001b[32m1\u001b[39m), dtype=dtype, device=device)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_steps\u001b[49m):\n\u001b[32m     23\u001b[39m     loss_val += loss(mem_rec[step], targets)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Gradient calculation + weight update\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'num_steps' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        for step in range(num_steps):\n",
    "            loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "            for step in range(num_steps):\n",
    "                test_loss += loss(test_mem[step], test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            if counter % 50 == 0:\n",
    "                train_printer()\n",
    "            counter += 1\n",
    "            iter_counter +=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
